<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Fire360: Robust Perception & Episodic Memory in 360¬∞ Firefighting Video</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet" />

  <style>
    :root {
      --bg: #050708;
      --card: #101418;
      --accent: #d8d8d8;
      --accent-soft: #a6a6a6;
      --gold: #f2c96a;
      --text-main: #f5f5f5;
      --text-muted: #a0a4aa;
      --border-subtle: rgba(200,200,200,0.12);
      --shadow-soft: 0 18px 40px rgba(0,0,0,0.55);
      --radius-lg: 18px;
      --radius-pill: 999px;
      --transition-fast: 0.22s ease;
    }

    * { box-sizing: border-box; margin: 0; padding: 0; }
    html { scroll-behavior: smooth; }

    body {
      font-family: "Inter", system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
      background: radial-gradient(circle at top, #15191f 0, #050708 55%);
      color: var(--text-main);
      -webkit-font-smoothing: antialiased;
      line-height: 1.6;
    }

    a { color: inherit; text-decoration: none; }

    .page {
      max-width: 1040px;
      margin: 0 auto;
      padding: 32px 18px 72px;
    }

    /* NAVBAR */

    .nav {
      position: sticky;
      top: 0;
      z-index: 30;
      backdrop-filter: blur(18px);
      background: linear-gradient(120deg, rgba(10,12,14,0.92), rgba(5,7,8,0.96));
      border-bottom: 1px solid rgba(255,255,255,0.04);
      padding: 10px 18px;
    }

    .nav-inner {
      max-width: 1040px;
      margin: 0 auto;
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 12px;
    }

    .nav-left {
      display: flex;
      align-items: center;
      gap: 10px;
      font-size: 0.82rem;
    }

    .nav-dot {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: radial-gradient(circle at 30% 30%, #ffffff, #bdbdbd);
      box-shadow: 0 0 12px rgba(240,240,240,0.9);
    }

    .nav-title {
      letter-spacing: 0.14em;
      text-transform: uppercase;
      color: var(--text-muted);
    }

    .nav-links {
      display: flex;
      gap: 16px;
      font-size: 0.8rem;
      letter-spacing: 0.14em;
      text-transform: uppercase;
    }

    .nav-links a {
      color: var(--text-muted);
      padding-bottom: 3px;
      position: relative;
    }

    .nav-links a::after {
      content: "";
      position: absolute;
      left: 0;
      bottom: 0;
      width: 0;
      height: 1px;
      background: var(--accent);
      transition: width var(--transition-fast);
    }

    .nav-links a:hover::after {
      width: 100%;
    }

    /* HERO */

    .hero {
      margin-top: 36px;
      text-align: center;
    }

    .hero-badge {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 4px 14px;
      border-radius: var(--radius-pill);
      border: 1px solid rgba(242,201,106,0.32);
      background: radial-gradient(circle at left, rgba(242,201,106,0.2), transparent 55%);
      font-size: 0.78rem;
      letter-spacing: 0.18em;
      text-transform: uppercase;
      color: var(--gold);
      margin-bottom: 14px;
    }

    .hero-title {
      font-size: clamp(2.2rem, 3.4vw, 2.8rem);
      line-height: 1.12;
      font-weight: 600;
      margin-bottom: 14px;
    }

    .hero-subtitle {
      font-size: 0.98rem;
      color: var(--text-muted);
      max-width: 40rem;
      margin: 0 auto 14px;
    }

    .hero-authors {
      font-size: 0.85rem;
      color: var(--text-muted);
      margin-bottom: 10px;
    }

    .hero-authors a {
      text-decoration: underline;
      text-decoration-style: dotted;
      text-underline-offset: 3px;
    }

    .hero-meta-row {
      font-size: 0.78rem;
      color: var(--accent-soft);
      letter-spacing: 0.16em;
      text-transform: uppercase;
      margin-bottom: 18px;
    }

    .hero-buttons {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 10px;
      margin-bottom: 26px;
    }

    .btn {
      border-radius: var(--radius-pill);
      padding: 9px 18px;
      font-size: 0.84rem;
      border: 1px solid var(--border-subtle);
      background: linear-gradient(135deg, #f3f3f3, #bababa);
      color: #050505;
      display: inline-flex;
      align-items: center;
      gap: 8px;
      cursor: pointer;
      transition: transform var(--transition-fast), box-shadow var(--transition-fast), background var(--transition-fast);
      box-shadow: 0 12px 28px rgba(0,0,0,0.65);
    }

    .btn span.icon { font-size: 0.9rem; }

    .btn:hover {
      transform: translateY(-1px);
      background: linear-gradient(135deg, #ffffff, #cdcdcd);
      box-shadow: 0 18px 40px rgba(0,0,0,0.75);
    }

    .btn-secondary {
      background: rgba(10,12,15,0.9);
      color: var(--accent-soft);
      box-shadow: none;
    }

    .btn-secondary:hover {
      background: rgba(20,23,27,0.95);
      color: var(--accent);
      box-shadow: 0 14px 34px rgba(0,0,0,0.45);
    }

    .hero-metrics {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 14px;
      margin-bottom: 18px;
      font-size: 0.82rem;
      color: var(--accent-soft);
    }

    .metric-pill {
      padding: 6px 12px;
      border-radius: var(--radius-pill);
      border: 1px solid var(--border-subtle);
      background: rgba(8,10,12,0.8);
    }

    /* SECTIONS */

    section { margin-top: 56px; }

    .section-block {
      /* lighter 3-tone silver/grey gradient */
      background: radial-gradient(circle at top, #434b57 0, #2a3039 45%, #181c23 80%);
      border-radius: 22px;
      padding: 22px 18px 24px;
      border: 1px solid rgba(255,255,255,0.055);
      box-shadow: 0 14px 32px rgba(0,0,0,0.55);
    }

    .section-header {
      margin-bottom: 14px;
    }

    .section-title {
      font-size: 1.05rem;
      letter-spacing: 0.18em;
      text-transform: uppercase;
      color: var(--accent);
    }

    /* we keep the class in case you want it later, but no text uses it now */
    .section-tagline {
      font-size: 0.9rem;
      color: var(--text-muted);
      margin-top: 6px;
      max-width: none;
    }

    .section-body {
      font-size: 0.94rem;
      color: var(--text-muted);
      max-width: none;
    }

    .card {
      background: radial-gradient(circle at top, #202530 0, #0b0e14 60%);
      border-radius: var(--radius-lg);
      border: 1px solid var(--border-subtle);
      padding: 18px 18px 16px;
      box-shadow: var(--shadow-soft);
      margin-top: 18px;
    }

    .card-title {
      font-size: 0.96rem;
      font-weight: 500;
      margin-bottom: 6px;
    }

    .card-subtitle {
      font-size: 0.8rem;
      letter-spacing: 0.16em;
      text-transform: uppercase;
      color: var(--accent-soft);
      margin-bottom: 4px;
    }

    /* FIGURES */

    .figure {
      margin-top: 16px;
      margin-bottom: 10px;
    }

    .figure img {
      width: 100%;
      border-radius: 14px;
      border: 1px solid rgba(255,255,255,0.04);
      display: block;
      background: #0b0e10;
    }

    .figure-caption {
      font-size: 0.8rem;
      color: var(--text-muted);
      margin-top: 6px;
      max-width: none;
    }

    /* TABLES */

    .table-wrapper {
      margin-top: 16px;
      border-radius: 18px;
      overflow: hidden;
      border: 1px solid var(--border-subtle);
      background: radial-gradient(circle at top, #171b20, #050708);
      box-shadow: var(--shadow-soft);
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.83rem;
    }

    th, td {
      padding: 9px 12px;
      white-space: nowrap;
    }

    th {
      text-align: left;
      text-transform: uppercase;
      letter-spacing: 0.12em;
      font-size: 0.74rem;
      color: var(--text-muted);
      border-bottom: 1px solid var(--border-subtle);
      background: rgba(4,5,7,0.96);
    }

    tbody tr {
      transition: background var(--transition-fast), transform var(--transition-fast), box-shadow var(--transition-fast);
    }

    tbody tr:nth-child(odd) {
      background: rgba(255,255,255,0.01);
    }

    tbody tr:hover {
      background: linear-gradient(90deg, rgba(242,201,106,0.18), rgba(255,255,255,0.02));
      transform: translateY(-1px);
      box-shadow: 0 10px 30px rgba(0,0,0,0.5);
    }

    .row-highlight {
      background: linear-gradient(90deg, rgba(242,201,106,0.16), rgba(0,0,0,0));
    }

    .row-group {
      background: rgba(0,0,0,0.75);
      font-weight: 500;
      text-transform: none;
      letter-spacing: 0.08em;
    }

    .cell-human { font-weight: 500; color: #ffffff; }
    .cell-metric { color: var(--accent-soft); }

    /* TASK CARDS */

    .task-grid {
      display: grid;
      grid-template-columns: repeat(2, minmax(0, 1fr));
      gap: 18px;
      margin-top: 10px;
    }

    @media (max-width: 800px) {
      .task-grid { grid-template-columns: 1fr; }
    }

    .task-card {
      background: linear-gradient(145deg, #15181d, #080a0c);
      border-radius: 16px;
      border: 1px solid var(--border-subtle);
      padding: 14px 14px 12px;
      position: relative;
      overflow: hidden;
    }

    .task-title {
      font-size: 0.94rem;
      font-weight: 500;
      margin-bottom: 6px;
    }

    .task-meta {
      font-size: 0.8rem;
      color: var(--text-muted);
      margin-bottom: 8px;
    }

    .task-text {
      font-size: 0.84rem;
      color: var(--text-muted);
    }

    .pill {
      display: inline-flex;
      align-items: center;
      gap: 6px;
      border-radius: var(--radius-pill);
      border: 1px solid var(--border-subtle);
      padding: 4px 9px;
      font-size: 0.74rem;
      color: var(--accent-soft);
      margin-top: 6px;
    }

    .pill-dot {
      width: 6px;
      height: 6px;
      border-radius: 50%;
      background: var(--gold);
    }

    /* CODE BLOCK */

    pre {
      font-family: "JetBrains Mono", ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 0.8rem;
      background: #050607;
      border-radius: 12px;
      border: 1px solid var(--border-subtle);
      padding: 12px 14px;
      overflow-x: auto;
      color: #d4d4d4;
    }

    /* FOOTER */

    .footer {
      margin-top: 52px;
      padding-top: 18px;
      border-top: 1px solid var(--border-subtle);
      font-size: 0.8rem;
      color: var(--text-muted);
      display: flex;
      flex-wrap: wrap;
      justify-content: space-between;
      gap: 8px;
    }

    /* REVEAL ANIMATION */

    .reveal {
      opacity: 0;
      transform: translateY(18px);
      transition: opacity 0.6s ease-out, transform 0.6s ease-out;
    }

    .reveal.visible {
      opacity: 1;
      transform: translateY(0);
    }
  </style>
</head>
<body>
  <!-- NAVBAR -->
  <header class="nav">
    <div class="nav-inner">
      <div class="nav-left">
        <div class="nav-dot"></div>
        <div class="nav-title">Fire360 </div>
      </div>
      <nav class="nav-links">
        <a href="#overview">Overview</a>
        <a href="#dataset">Dataset</a>
        <a href="#tasks">Tasks</a>
        <a href="#tor">TOR</a>
        <a href="#results">Results</a>
        <a href="#resources">Resources</a>
      </nav>
    </div>
  </header>

  <main class="page">
    <!-- HERO -->
    <section class="hero reveal">
      <div class="hero-badge">
        <span>NeurIPS 2025 Spotlight ¬∑ Dataset & Benchmark</span>
      </div>
      <h1 class="hero-title">
        Fire360: Robust Perception & Episodic Memory in Degraded 360¬∞ Firefighting Video
      </h1>
      <p class="hero-subtitle">
        A large-scale benchmark of 360¬∞ firefighter training videos that probes spatial grounding,
        temporal understanding, safety-critical reasoning, and transformation-invariant retrieval
        in the environments where reliability matters most.
      </p>
      <p class="hero-authors">
        <a href="https://adititiwari19.github.io/" target="_blank" rel="noreferrer">Aditi Tiwari</a>,
        Farzaneh Masoud, Dac Trong Nguyen, Jill Kraft,
        <a href="https://siebelschool.illinois.edu/about/people/faculty/hengji" target="_blank" rel="noreferrer">Heng Ji</a>,
        <a href="https://siebelschool.illinois.edu/about/people/faculty/klara" target="_blank" rel="noreferrer">Klara Nahrstedt</a><br/>
        University of Illinois Urbana‚ÄìChampaign ¬∑ Illinois Fire Service Institute
      </p>
      <div class="hero-meta-row">
        228 videos ¬∑ 50 hours ¬∑ 5 tasks ¬∑ 43.7-point human‚Äìmodel gap on TOR
      </div>

      <div class="hero-buttons">
        <a class="btn" href="https://arxiv.org/abs/2506.02167" target="_blank" rel="noreferrer">
          <span class="icon">‚¨á</span><span>Paper (arXiv)</span>
        </a>
        <a class="btn btn-secondary" href="https://uofi.app.box.com/v/fire360dataset" target="_blank" rel="noreferrer">
          <span class="icon">üéû</span><span>Dataset Download</span>
        </a>
        <a class="btn btn-secondary" href="images/poster.pdf" target="_blank" rel="noreferrer">
          <span class="icon">üñº</span><span>Poster (PDF)</span>
        </a>
        <a class="btn btn-secondary" href="https://neurips.cc/virtual/2025/loc/san-diego/poster/121673" target="_blank" rel="noreferrer">
          <span class="icon">‚≠ê</span><span>NeurIPS Page</span>
        </a>
      </div>
<!-- 
      <div class="hero-metrics">
        <div class="metric-pill">360¬∞ real-world firefighter drills</div>
        <div class="metric-pill">Dataset + benchmark suite</div>
        <div class="metric-pill">Safety-critical perception & memory</div>
      </div> -->
    </section>
<!-- OVERVIEW -->
<section id="overview" class="reveal section-block">
  <div class="section-header">
    <h2 class="section-title">Overview</h2>
  </div>
  <div class="section-body">
    <p>
      Each year, tens of thousands of firefighters are injured in the line of duty, often when
      visibility is severely reduced and situational perception breaks down. Existing video
      benchmarks typically assume clean, forward-facing views or synthetic scenes, which makes it
      difficult to study model behavior in degraded 360¬∞ environments where decisions have
      direct safety implications.
    </p>
    <br/>
    <p>
      Fire360 is constructed from professionally recorded firefighter training sessions with
      certified instructors and covers indoor rescues and outdoor operations across day, night,
      dense smoke, and post-fire overhaul. The benchmark defines five tasks
      (visual question answering, temporal captioning, object localization, safety-critical
      reasoning, and transformed object retrieval) in order to isolate where human experts retain
      robust understanding and state-of-the-art multimodal models still fail.
    </p>
  </div>
</section>


    <!-- DATASET -->
    <section id="dataset" class="reveal section-block">
      <div class="section-header">
        <h2 class="section-title">Dataset</h2>
      </div>

      <div class="figure">
        <img src="images/fire360_example_frames.png"
             alt="Example frames from Fire360: outdoor day, night operation, zero visibility, indoor drills, charred scenes, and smoke obstruction." />
        <div class="figure-caption">
          Figure 1: Example Fire360 frames across outdoor day/night, zero-visibility,
          indoor charred scenes, and smoke-obstructed training environments. Replace this image
          with your actual montage.
        </div>
      </div>

      <div class="card">
        <div class="card-subtitle">Positioning</div>
        <div class="card-title">Comparison with existing video datasets</div>
        <p class="section-body" style="margin-top: 6px;">
          Fire360 complements prior video resources by jointly offering 360¬∞ views, egocentric and
          third-person perspectives, synchronized audio, and explicitly safety-critical events.
        </p>

        <div class="table-wrapper">
          <table>
            <thead>
              <tr>
                <th>Dataset</th>
                <th>Third-Person</th>
                <th>360¬∞</th>
                <th>Egocentric</th>
                <th>Video</th>
                <th>Audio</th>
                <th>Real-world</th>
                <th>Safety-Critical</th>
                <th>Duration (s)</th>
                <th>Public</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Ego4D</td>
                <td>‚úó</td><td>‚úó</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>10,800,000</td><td>‚úì</td>
              </tr>
              <tr>
                <td>EPIC-Kitchens</td>
                <td>‚úó</td><td>‚úó</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úì</td><td>‚úó</td><td>712,800</td><td>‚úì</td>
              </tr>
              <tr>
                <td>360+x</td>
                <td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>244,800</td><td>‚úì</td>
              </tr>
              <tr>
                <td>HACS++</td>
                <td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>500,400</td><td>‚úì</td>
              </tr>
              <tr class="row-highlight">
                <td><strong>Fire360 (Ours)</strong></td>
                <td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td><strong>180,000</strong></td><td>‚úì</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>

      <div class="card">
        <div class="card-subtitle">Content Distribution</div>
        <div class="card-title">Scene and action statistics</div>
        <div class="figure">
          <img src="images/fire360_dataset_stats.png"
               alt="Donut charts for Fire360 scene categories (indoor vs outdoor day/night) and action categories with counts and percentages." />
          <div class="figure-caption">
            Figure 3: Scene distribution, action categories and object categories.
          </div>
        </div>
        <p class="section-body">
          Fire360 balances <strong>indoor rescue/training (43.9%)</strong> with
          <strong>outdoor operations at night (28.5%)</strong> and
          <strong>outdoor operations during the day (27.6%)</strong>, ensuring that models must
          handle both low-light interiors and open-air incident grounds. Eight safety-relevant
          action categories support temporal reasoning and analysis of model failure modes.
        </p>
      </div>
    </section>

    <!-- TASKS -->
    <section id="tasks" class="reveal section-block">
      <div class="section-header">
        <h2 class="section-title">Benchmark Tasks</h2>
      </div>

      <div class="task-grid">
        <div class="task-card">
          <div class="task-title">360¬∞ Visual Question Answering (VQA)</div>
          <div class="task-meta">Spatial reasoning ¬∑ Checklist queries</div>
          <p class="task-text">
            Models answer expert-authored questions about presence, visibility, and protocol
            adherence in single equirectangular frames
            (e.g., ‚ÄúIs a clear egress path visible through the smoke?‚Äù).
          </p>
          <div class="pill"><span class="pill-dot"></span>Metric: Top-1 accuracy</div>
        </div>

        <div class="task-card">
          <div class="task-title">Temporal Action Captioning</div>
          <div class="task-meta">Procedure summarization ¬∑ 10‚Äì20s clips</div>
          <p class="task-text">
            Given short degraded clips, models must describe what firefighters are doing in
            natural language, aligned with standardized training procedures.
          </p>
          <div class="pill"><span class="pill-dot"></span>Metric: BLEU-4</div>
        </div>

        <div class="task-card">
          <div class="task-title">Object Localization under Distortion</div>
          <div class="task-meta">Detection ¬∑ Smoke & geometric stretch</div>
          <p class="task-text">
            Category-agnostic localization of safety gear (SCBA, helmets, hoses) in 360¬∞
            equirectangular frames, where poles exhibit strong geometric distortion.
          </p>
          <div class="pill"><span class="pill-dot"></span>Metric: Mean IoU</div>
        </div>

        <div class="task-card">
          <div class="task-title">Safety-Critical Reasoning</div>
          <div class="task-meta">Checklist-based compliance</div>
          <p class="task-text">
            Models decide whether a frame/clip is safe and justify their decision using a
            firefighter checklist (e.g., sealed gas mask, three-point contact on ladders).
          </p>
          <div class="pill"><span class="pill-dot"></span>Metric: Checklist accuracy</div>
        </div>

        <div class="task-card">
          <div class="task-title">Transformed Object Retrieval (TOR)</div>
          <div class="task-meta">Episodic memory ¬∑ Core contribution</div>
          <p class="task-text">
            Given a clean exemplar of an object (helmet, hose, extinguisher) and an unpaired 360¬∞
            scene after fire damage, the model must retrieve the degraded instance without seeing
            the transformation.
          </p>
          <div class="pill"><span class="pill-dot"></span>Metric: Top-1 retrieval accuracy</div>
        </div>
      </div>
    </section>

    <!-- TOR SECTION -->
    <section id="tor" class="reveal section-block">
      <div class="section-header">
        <h2 class="section-title">Transformed Object Retrieval (TOR)</h2>
      </div>

      <div class="figure">
        <img src="images/fire360_tor_diagram.png"
             alt="Diagram of the TOR task: 360¬∞ frames, a reference catalogue of pristine objects, and retrieval of the degraded object region." />
        <div class="figure-caption">
          Figure 6: Illustration of TOR. A world model with episodic memory receives
          (i) 360¬∞ frames from a post-fire scene and (ii) a catalogue of pristine reference
          objects. Given a natural-language query such as ‚ÄúLocate the red firefighter helmet in the
          post-fire scene,‚Äù the model must mark the correct degraded helmet region.
        </div>
      </div>

      <div class="card">
        <div class="card-subtitle">Example</div>
        <div class="card-title">From pristine helmet to charred silhouette</div>
        <p class="section-body">
          In a typical example, the reference catalogue provides a clean side view of a red
          firefighter helmet. The target frame shows the same helmet after an interior room burn:
          the visor is blackened, reflections are gone, and surrounding walls have collapsed.
          Pipes, lamps, and other round shapes now compete as distractors.
        </p>
        <br/>
        <p class="section-body">
          Humans successfully retrieve the correct region in <strong>83.5%</strong> of cases.
          GPT-4o achieves only <strong>39.8%</strong>, often confusing pipes or background
          clutter for the helmet. CLIP and BLIP-2 perform worse, revealing that current models
          lack transformation-invariant object identity when temporal continuity is removed.
        </p>
      </div>
    </section>

    <section id="results" class="reveal section-block">
      <div class="section-header">
        <h2 class="section-title">Results</h2>
        <p class="section-tagline">
          Bar plots summarize the persistent human‚Äìmodel gaps across all five Fire360 tasks.
          Even state-of-the-art systems fall far short of expert performance when faced with
          smoke, darkness, and 360¬∞ distortion.
        </p>
      </div>
    
      <!-- Simple styles for grid + dropdown + centered TOR -->
      <style>
        .results-toggle {
          margin-bottom: 20px;
        }
    
        .results-toggle summary {
          cursor: pointer;
          font-weight: 600;
          list-style: none;
        }
    
        .results-toggle summary::-webkit-details-marker {
          display: none;
        }
    
        .results-toggle summary::before {
          content: "‚ñ∏ ";
          font-size: 0.9em;
        }
    
        .results-toggle[open] summary::before {
          content: "‚ñæ ";
        }
    
        .results-grid {
          display: grid;
          grid-template-columns: repeat(2, minmax(0, 1fr));
          gap: 24px;
          margin-top: 10px;
        }
    
        .results-grid .figure {
          margin: 0;
        }
    
        .figure-tor {
          max-width: 650px;
          margin: 32px auto 0 auto;
        }
    
        .figure-tor img {
          display: block;
          width: 100%;
          height: auto;
        }
    
        .figure-caption {
          margin-top: 8px;
          font-size: 0.9rem;
          opacity: 0.85;
        }
    
        .table-wrapper {
          overflow-x: auto;
        }
    
        .row-group td {
          font-weight: 600;
          padding-top: 10px;
          border-top: 1px solid rgba(255, 255, 255, 0.08);
        }
    
        /* Soft golden highlight for best model rows (Option A) */
        .row-highlight {
          background: rgba(255, 215, 0, 0.18); /* soft golden overlay */
        }
    
        .row-highlight td {
          font-weight: 600;
          color: #ffd46b; /* warm gold text */
        }
    
        @media (max-width: 768px) {
          .results-grid {
            grid-template-columns: 1fr;
          }
          .figure-tor {
            margin-top: 24px;
          }
        }
      </style>
    
      <!-- DROPDOWN: merged tabular results -->
      <details class="results-toggle">
        <summary>View detailed tabular results</summary>
    
        <div class="card" style="margin-top: 12px;">
          <div class="card-subtitle">All Models</div>
          <div class="card-title">Performance across Fire360 tasks</div>
          <p class="section-body" style="margin-top: 6px;">
            For each task, the highlighted row indicates the best-performing model across all evaluated systems.
          </p>
    
          <div class="table-wrapper" style="margin-top: 10px;">
            <table>
              <thead>
                <tr>
                  <th>Model</th>
                  <th>Score</th>
                  <th>Human</th>
                  <th>Metric</th>
                </tr>
              </thead>
              <tbody>
                <!-- VQA -->
                <tr class="row-group">
                  <td colspan="4">Task: Visual Question Answering (VQA)</td>
                </tr>
                <tr class="row-highlight">
                  <td>GPT-4o</td>
                  <td>53.8%</td>
                  <td class="cell-human">91.4%</td>
                  <td class="cell-metric">Top-1 accuracy</td>
                </tr>
                <tr>
                  <td>Qwen-VL</td>
                  <td>47.2%</td>
                  <td class="cell-human">91.4%</td>
                  <td class="cell-metric">Top-1 accuracy</td>
                </tr>
                <tr>
                  <td>LLaVA-v1.5-13B</td>
                  <td>50.3%</td>
                  <td class="cell-human">91.4%</td>
                  <td class="cell-metric">Top-1 accuracy</td>
                </tr>
                <tr>
                  <td>BLIP-2 (OPT-6.7B)</td>
                  <td>42.7%</td>
                  <td class="cell-human">91.4%</td>
                  <td class="cell-metric">Top-1 accuracy</td>
                </tr>
                <tr>
                  <td>InstructBLIP</td>
                  <td>48.6%</td>
                  <td class="cell-human">91.4%</td>
                  <td class="cell-metric">Top-1 accuracy</td>
                </tr>
                <tr>
                  <td>Kosmos-2.5</td>
                  <td>47.5%</td>
                  <td class="cell-human">91.4%</td>
                  <td class="cell-metric">Top-1 accuracy</td>
                </tr>
    
                <!-- TAC -->
                <tr class="row-group">
                  <td colspan="4">Task: Temporal Action Captioning</td>
                </tr>
                <tr class="row-highlight">
                  <td>GLaMM-7B</td>
                  <td>0.341</td>
                  <td class="cell-human">0.85</td>
                  <td class="cell-metric">BLEU-4</td>
                </tr>
                <tr>
                  <td>SwinBERT</td>
                  <td>0.315</td>
                  <td class="cell-human">0.85</td>
                  <td class="cell-metric">BLEU-4</td>
                </tr>
                <tr>
                  <td>ProgressCaptioner</td>
                  <td>0.288</td>
                  <td class="cell-human">0.85</td>
                  <td class="cell-metric">BLEU-4</td>
                </tr>
    
                <!-- Object Localization -->
                <tr class="row-group">
                  <td colspan="4">Task: Object Localization under Distortion</td>
                </tr>
                <tr>
                  <td>Grounding DINO</td>
                  <td>38.4%</td>
                  <td class="cell-human">85.2%</td>
                  <td class="cell-metric">Mean IoU</td>
                </tr>
                <tr class="row-highlight">
                  <td>OWLv2</td>
                  <td>39.8%</td>
                  <td class="cell-human">85.2%</td>
                  <td class="cell-metric">Mean IoU</td>
                </tr>
                <tr>
                  <td>YOLO-World</td>
                  <td>36.5%</td>
                  <td class="cell-human">85.2%</td>
                  <td class="cell-metric">Mean IoU</td>
                </tr>
    
                <!-- Safety-Critical Reasoning -->
                <tr class="row-group">
                  <td colspan="4">Task: Safety-Critical Reasoning</td>
                </tr>
                <tr>
                  <td>GPT-4o (prompted)</td>
                  <td>28.9%</td>
                  <td class="cell-human">94.6%</td>
                  <td class="cell-metric">Checklist accuracy</td>
                </tr>
                <tr>
                  <td>Qwen-VL</td>
                  <td>32.5%</td>
                  <td class="cell-human">94.6%</td>
                  <td class="cell-metric">Checklist accuracy</td>
                </tr>
                <tr class="row-highlight">
                  <td>Claude-3 Sonnet</td>
                  <td>33.0%</td>
                  <td class="cell-human">94.6%</td>
                  <td class="cell-metric">Checklist accuracy</td>
                </tr>
                <tr>
                  <td>Llama-Guard-3-8B</td>
                  <td>27.4%</td>
                  <td class="cell-human">94.6%</td>
                  <td class="cell-metric">Checklist accuracy</td>
                </tr>
    
                <!-- TOR -->
                <tr class="row-group">
                  <td colspan="4">Task: Transformed Object Retrieval (TOR)</td>
                </tr>
                <tr class="row-highlight">
                  <td>GPT-4o</td>
                  <td>39.8%</td>
                  <td class="cell-human">83.5%</td>
                  <td class="cell-metric">Retrieval accuracy</td>
                </tr>
                <tr>
                  <td>CLIP (ViT-B/32)</td>
                  <td>32.5%</td>
                  <td class="cell-human">83.5%</td>
                  <td class="cell-metric">Retrieval accuracy</td>
                </tr>
                <tr>
                  <td>BLIP-2 (OPT-6.7B)</td>
                  <td>35.1%</td>
                  <td class="cell-human">83.5%</td>
                  <td class="cell-metric">Retrieval accuracy</td>
                </tr>
                <tr>
                  <td>CoLLM</td>
                  <td>35.7%</td>
                  <td class="cell-human">83.5%</td>
                  <td class="cell-metric">Retrieval accuracy</td>
                </tr>
                <tr>
                  <td>MCoT-RE</td>
                  <td>33.5%</td>
                  <td class="cell-human">83.5%</td>
                  <td class="cell-metric">Retrieval accuracy</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
      </details>
    
      <!-- 2√ó2 grid of core task plots -->
      <div class="results-grid">
        <!-- VQA -->
        <div class="figure">
          <img src="images/vqa.png"
               alt="Bar chart comparing VQA performance of GPT-4o, Qwen-VL, LLaVA-13B, BLIP-2, InstructBLIP, Kosmos-2.5, and human experts." />
          <div class="figure-caption">
            Visual Question Answering (VQA): The best model still lags human experts by
            38‚Äì49 accuracy points, showing that basic situational questions remain unreliable under
            smoke and occlusion.
          </div>
        </div>
    
        <!-- Temporal Action Captioning -->
        <div class="figure">
          <img src="images/temporal_action_caption.png"
               alt="Bar chart comparing temporal action captioning performance of GLaMM-7B, SwinBERT, ProgressCaptioner, and human experts." />
          <div class="figure-caption">
            Temporal Action Captioning: Video captioning models capture rough activity but miss key
            safety-relevant details, with BLEU-4 gaps of roughly 0.5‚Äì0.56 to human experts.
          </div>
        </div>
    
        <!-- Object Localization -->
        <div class="figure">
          <img src="images/object_localization.png"
               alt="Bar chart comparing object localization performance of Grounding DINO, OWLv2, YOLO-World, and human experts." />
          <div class="figure-caption">
            Object Localization under Distortion: Detectors lose nearly half of the mean IoU compared
            to humans when safety gear appears in distorted or smoke-obscured regions of the 360¬∞ frame.
          </div>
        </div>
    
        <!-- Safety-Critical Reasoning -->
        <div class="figure">
          <img src="images/safety_critical_reasoning.png"
               alt="Bar chart comparing safety-critical reasoning performance of GPT-4o, Claude-3 Sonnet, Llama-Guard-3-8B, Qwen-VL, and human experts." />
          <div class="figure-caption">
            Safety-Critical Reasoning: Dedicated safety and policy models still trail experts by
            60‚Äì67 checklist-accuracy points, frequently missing violations obvious to trained instructors.
          </div>
        </div>
      </div>
    
      <!-- Centered TOR plot -->
      <div class="figure figure-tor">
        <img src="images/tor.png"
             alt="Bar chart comparing transformed object retrieval performance of GPT-4o, CLIP, BLIP-2, CoLLM, MCoT-RE, and human experts." />
        <div class="figure-caption">
          Transformed Object Retrieval (TOR): All evaluated models remain 44‚Äì51 points below human
          retrieval accuracy, confirming that current representations do not preserve object identity
          under severe fire damage.
        </div>
      </div>
    
      <!-- VQA degradation heatmaps (complementary view) -->
      <!-- <div class="figure" style="margin-top: 32px;">
        <img src="images/fire360_vqa_heatmap.png"
             alt="Three heatmaps showing GPT-4o model performance, human performance, and their accuracy gap across smoke and light levels." />
        <div class="figure-caption">
          Degradation-aware VQA: Heatmaps show GPT-4o‚Äôs accuracy collapsing with higher smoke density
          and lower light, while human experts stay above 80% across the grid.
        </div>
      </div> -->
    
      <p class="section-body">
        Together, these plots and tables highlight a consistent pattern: Fire360 exposes large,
        degradation-driven failures across architectures and tasks, indicating that robust perception
        and episodic memory in 360¬∞ emergency scenes remain very open problems for current multimodal models.
      </p>
    </section>
    

<!-- RESOURCES -->
<section id="resources" class="reveal section-block">
  <div class="section-header">
    <h2 class="section-title">Ethics and Bibtex</h2>
  </div>

  <div class="card">
    <div class="card-subtitle">Ethics</div>
    <div class="card-title">Responsible release</div>
    <p class="section-body" style="margin-top: 6px;">
      All footage documents professional training drills with informed consent. No personally
      identifiable information is included; participants appear in protective gear. Fire360 is
      released for research on robust, safety-critical multimodal systems and may not be used
      for surveillance, profiling, or non-consensual monitoring.
    </p>
  </div>

  <div class="card">
    <div class="card-subtitle">Citation</div>
    <div class="card-title">BibTeX</div>
    <pre>
@misc{tiwari2025fire360benchmarkrobustperception,
  title         = {Fire360: A Benchmark for Robust Perception and Episodic Memory
                   in Degraded 360-Degree Firefighting Videos},
  author        = {Aditi Tiwari and Farzaneh Masoud and Dac Trong Nguyen
                   and Jill Kraft and Heng Ji and Klara Nahrstedt},
  year          = {2025},
  eprint        = {2506.02167},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2506.02167},
}</pre>
  </div>
</section>

<!-- FOOTER -->
<footer class="footer">
  <div>
    Fire360 ¬∑ NeurIPS 2025 Spotlight &nbsp;¬∑&nbsp; UIUC ¬∑ Illinois Fire Service Institute
  </div>
  <div>
    Contact: <a href="mailto:aditit5@illinois.edu">aditit5@illinois.edu</a>
  </div>
</footer>

  </main>

  <script>
    // Reveal elements on scroll
    const reveals = document.querySelectorAll('.reveal');
    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          entry.target.classList.add('visible');
          observer.unobserve(entry.target);
        }
      });
    }, { threshold: 0.15 });

    reveals.forEach(el => observer.observe(el));
  </script>
</body>
</html>
