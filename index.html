<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Fire360: Robust Perception & Episodic Memory in 360¬∞ Firefighting Video</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- Minimal, modern font -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet" />

  <style>
    :root {
      --bg: #050708;
      --bg-alt: #0c0f11;
      --card: #111417;
      --accent: #d8d8d8;
      --accent-soft: #a5a5a5;
      --text-main: #f5f5f5;
      --text-muted: #a0a4aa;
      --border-subtle: rgba(180,180,180,0.1);
      --shadow-soft: 0 18px 45px rgba(0,0,0,0.55);
      --radius-lg: 18px;
      --radius-pill: 999px;
      --transition-fast: 0.25s ease;
    }

    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    html {
      scroll-behavior: smooth;
    }

    body {
      font-family: "Inter", system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
      background: radial-gradient(circle at top, #1b1e22 0, #050708 50%);
      color: var(--text-main);
      line-height: 1.6;
      -webkit-font-smoothing: antialiased;
    }

    a {
      color: inherit;
      text-decoration: none;
    }

    .page {
      max-width: 1120px;
      margin: 0 auto;
      padding: 32px 20px 72px;
    }

    /* Top nav */
    .nav {
      position: sticky;
      top: 0;
      z-index: 50;
      backdrop-filter: blur(18px);
      background: radial-gradient(circle at top left, rgba(70,70,70,0.15), rgba(5,7,8,0.9));
      border-radius: 999px;
      border: 1px solid var(--border-subtle);
      padding: 10px 20px;
      margin: 0 auto 24px;
      max-width: 1120px;
      display: flex;
      align-items: center;
      justify-content: space-between;
      box-shadow: 0 10px 40px rgba(0,0,0,0.45);
    }

    .nav-left {
      display: flex;
      align-items: center;
      gap: 10px;
    }

    .nav-dot {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: radial-gradient(circle at 30% 30%, #ffffff, #bbbbbb);
      box-shadow: 0 0 12px rgba(230,230,230,0.9);
    }

    .nav-title {
      font-size: 0.9rem;
      letter-spacing: 0.12em;
      text-transform: uppercase;
      color: var(--text-muted);
    }

    .nav-links {
      display: flex;
      gap: 18px;
      font-size: 0.82rem;
      text-transform: uppercase;
      letter-spacing: 0.16em;
    }

    .nav-links a {
      color: var(--text-muted);
      position: relative;
      padding-bottom: 4px;
      cursor: pointer;
    }

    .nav-links a::after {
      content: "";
      position: absolute;
      left: 0;
      bottom: 0;
      width: 0;
      height: 1px;
      background: var(--accent);
      transition: width var(--transition-fast);
    }

    .nav-links a:hover::after {
      width: 100%;
    }

    /* Hero */
    .hero {
      display: grid;
      grid-template-columns: minmax(0, 1.6fr) minmax(0, 1.1fr);
      gap: 36px;
      margin-top: 32px;
      align-items: center;
    }

    @media (max-width: 880px) {
      .hero {
        grid-template-columns: 1fr;
      }
    }

    .badge {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 4px 12px;
      border-radius: var(--radius-pill);
      border: 1px solid var(--border-subtle);
      background: radial-gradient(circle at left, rgba(255,255,255,0.12), transparent 60%);
      font-size: 0.75rem;
      text-transform: uppercase;
      letter-spacing: 0.14em;
      color: var(--accent-soft);
      margin-bottom: 12px;
    }

    .badge-dot {
      width: 6px;
      height: 6px;
      border-radius: 50%;
      background: #d6d6d6;
    }

    .hero-title {
      font-size: clamp(2.1rem, 3.2vw, 2.7rem);
      line-height: 1.08;
      font-weight: 600;
      margin-bottom: 14px;
    }

    .hero-subtitle {
      font-size: 0.98rem;
      color: var(--text-muted);
      max-width: 34rem;
      margin-bottom: 18px;
    }

    .hero-authors {
      font-size: 0.85rem;
      color: var(--text-muted);
      margin-bottom: 18px;
    }

    .hero-buttons {
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
      margin-bottom: 16px;
    }

    .btn {
      border-radius: var(--radius-pill);
      padding: 9px 18px;
      font-size: 0.84rem;
      border: 1px solid var(--border-subtle);
      background: linear-gradient(135deg, #f3f3f3, #b8b8b8);
      color: #050505;
      display: inline-flex;
      align-items: center;
      gap: 8px;
      cursor: pointer;
      transition: transform var(--transition-fast), box-shadow var(--transition-fast), background var(--transition-fast);
      box-shadow: 0 12px 30px rgba(0,0,0,0.65);
    }

    .btn span.icon {
      font-size: 0.9rem;
    }

    .btn:hover {
      transform: translateY(-1px);
      box-shadow: 0 18px 40px rgba(0,0,0,0.75);
      background: linear-gradient(135deg, #ffffff, #cbcbcb);
    }

    .btn-secondary {
      background: rgba(15,18,21,0.9);
      color: var(--accent-soft);
      border: 1px solid var(--border-subtle);
      box-shadow: none;
    }

    .btn-secondary:hover {
      background: rgba(30,34,38,0.95);
      box-shadow: 0 14px 34px rgba(0,0,0,0.45);
      color: var(--accent);
    }

    .hero-meta {
      font-size: 0.78rem;
      color: var(--text-muted);
      text-transform: uppercase;
      letter-spacing: 0.18em;
    }

    .hero-right {
      position: relative;
    }

    .hero-card {
      background: radial-gradient(circle at top, #1b1f23 0, #090b0d 60%);
      border-radius: 30px;
      border: 1px solid var(--border-subtle);
      padding: 18px 18px 16px;
      box-shadow: var(--shadow-soft);
      overflow: hidden;
      position: relative;
    }

    .hero-card-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 10px;
      font-size: 0.8rem;
      color: var(--text-muted);
    }

    .hero-card-chip {
      padding: 4px 10px;
      border-radius: var(--radius-pill);
      border: 1px solid var(--border-subtle);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.16em;
      color: var(--accent-soft);
    }

    .hero-view {
      position: relative;
      background: radial-gradient(circle at center, #2f343b 0, #050708 70%);
      border-radius: 24px;
      height: 220px;
      overflow: hidden;
      display: flex;
      align-items: center;
      justify-content: center;
      isolation: isolate;
    }

    .hero-view-inner {
      width: 96%;
      height: 70%;
      border-radius: 999px;
      border: 1px solid rgba(255,255,255,0.03);
      background: radial-gradient(circle at center, #202429, #050708);
      position: relative;
      overflow: hidden;
    }

    .hero-view-scan {
      position: absolute;
      left: -20%;
      right: -20%;
      height: 26%;
      background: linear-gradient(
        to bottom,
        rgba(255,255,255,0) 0,
        rgba(255,255,255,0.16) 50%,
        rgba(255,255,255,0) 100%
      );
      animation: scan 5s linear infinite;
      mix-blend-mode: screen;
      opacity: 0.4;
    }

    .hero-view-grid {
      position: absolute;
      inset: 0;
      background-image:
        linear-gradient(to right, rgba(255,255,255,0.05) 1px, transparent 1px),
        linear-gradient(to bottom, rgba(255,255,255,0.05) 1px, transparent 1px);
      background-size: 32px 32px;
      opacity: 0.35;
      transform: translateY(-3px);
    }

    .hero-view-label {
      position: absolute;
      right: 18px;
      top: 16px;
      background: rgba(0,0,0,0.55);
      padding: 6px 10px;
      border-radius: var(--radius-pill);
      font-size: 0.73rem;
      letter-spacing: 0.16em;
      text-transform: uppercase;
      color: var(--accent-soft);
      border: 1px solid var(--border-subtle);
    }

    .hero-stats {
      display: flex;
      justify-content: space-between;
      margin-top: 12px;
      gap: 10px;
      font-size: 0.75rem;
    }

    .hero-stat-item {
      flex: 1;
      border-radius: 12px;
      border: 1px solid var(--border-subtle);
      padding: 7px 10px;
      background: radial-gradient(circle at top left, rgba(60,60,60,0.2), rgba(10,10,10,0.9));
    }

    .hero-stat-label {
      color: var(--text-muted);
      font-size: 0.72rem;
      text-transform: uppercase;
      letter-spacing: 0.16em;
      margin-bottom: 4px;
    }

    .hero-stat-value {
      font-size: 0.9rem;
      font-weight: 500;
    }

    @keyframes scan {
      0% { transform: translateY(-50%); }
      50% { transform: translateY(120%); }
      100% { transform: translateY(-50%); }
    }

    /* Sections */
    section {
      margin-top: 64px;
    }

    .section-header {
      display: flex;
      justify-content: space-between;
      align-items: baseline;
      margin-bottom: 18px;
      gap: 16px;
    }

    .section-title {
      font-size: 1.1rem;
      text-transform: uppercase;
      letter-spacing: 0.16em;
      color: var(--accent);
    }

    .section-tagline {
      font-size: 0.86rem;
      color: var(--text-muted);
      max-width: 420px;
    }

    .section-body {
      font-size: 0.94rem;
      color: var(--text-muted);
    }

    .columns-two {
      display: grid;
      grid-template-columns: minmax(0, 1.4fr) minmax(0, 1.2fr);
      gap: 28px;
    }

    @media (max-width: 880px) {
      .columns-two {
        grid-template-columns: 1fr;
      }
    }

    .card {
      background: radial-gradient(circle at top, #171b20 0, #07090b 60%);
      border-radius: var(--radius-lg);
      border: 1px solid var(--border-subtle);
      padding: 18px 18px 16px;
      box-shadow: var(--shadow-soft);
      position: relative;
      overflow: hidden;
    }

    .card-subtitle {
      font-size: 0.8rem;
      text-transform: uppercase;
      letter-spacing: 0.16em;
      color: var(--accent-soft);
      margin-bottom: 8px;
    }

    .card-title {
      font-size: 0.98rem;
      font-weight: 500;
      margin-bottom: 8px;
    }

    .card-text {
      font-size: 0.88rem;
      color: var(--text-muted);
    }

    .pill {
      display: inline-flex;
      align-items: center;
      gap: 6px;
      border-radius: var(--radius-pill);
      border: 1px solid var(--border-subtle);
      padding: 4px 10px;
      font-size: 0.73rem;
      color: var(--text-muted);
    }

    .pill-dot {
      width: 7px;
      height: 7px;
      border-radius: 50%;
      background: #d2d2d2;
    }

    /* Task grid */
    .task-grid {
      display: grid;
      grid-template-columns: repeat(3, minmax(0, 1fr));
      gap: 18px;
      margin-top: 12px;
    }

    @media (max-width: 920px) {
      .task-grid {
        grid-template-columns: repeat(2, minmax(0, 1fr));
      }
    }

    @media (max-width: 640px) {
      .task-grid {
        grid-template-columns: 1fr;
      }
    }

    .task-card {
      background: linear-gradient(150deg, #13161a, #07090b);
      border-radius: 16px;
      border: 1px solid var(--border-subtle);
      padding: 14px 14px 12px;
      position: relative;
      overflow: hidden;
      transition: transform var(--transition-fast), box-shadow var(--transition-fast), border-color var(--transition-fast);
    }

    .task-card::before {
      content: "";
      position: absolute;
      inset: -40%;
      background: radial-gradient(circle at top, rgba(255,255,255,0.12), transparent 60%);
      opacity: 0;
      transition: opacity var(--transition-fast);
    }

    .task-card:hover {
      transform: translateY(-3px);
      box-shadow: 0 18px 45px rgba(0,0,0,0.7);
      border-color: rgba(255,255,255,0.14);
    }

    .task-card:hover::before {
      opacity: 1;
    }

    .task-title {
      font-size: 0.92rem;
      font-weight: 500;
      margin-bottom: 4px;
    }

    .task-meta {
      font-size: 0.75rem;
      color: var(--text-muted);
      margin-bottom: 8px;
    }

    .task-text {
      font-size: 0.82rem;
      color: var(--text-muted);
      margin-bottom: 8px;
    }

    .task-chip-row {
      display: flex;
      flex-wrap: wrap;
      gap: 6px;
      font-size: 0.72rem;
    }

    /* TOR highlight */
    .tor-section {
      margin-top: 48px;
    }

    .tor-layout {
      display: grid;
      grid-template-columns: minmax(0, 1.2fr) minmax(0, 1.1fr);
      gap: 26px;
      align-items: stretch;
    }

    @media (max-width: 880px) {
      .tor-layout {
        grid-template-columns: 1fr;
      }
    }

    .tor-diagram {
      border-radius: 22px;
      border: 1px solid var(--border-subtle);
      padding: 16px;
      background: radial-gradient(circle at top, #171b20, #07090b);
      box-shadow: var(--shadow-soft);
      position: relative;
      overflow: hidden;
    }

    .tor-diagram-header {
      font-size: 0.8rem;
      text-transform: uppercase;
      letter-spacing: 0.18em;
      color: var(--accent-soft);
      margin-bottom: 10px;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    .tor-flow {
      display: grid;
      grid-template-columns: repeat(3, minmax(0, 1fr));
      gap: 10px;
      margin-top: 16px;
    }

    @media (max-width: 640px) {
      .tor-flow {
        grid-template-columns: 1fr;
      }
    }

    .tor-node {
      border-radius: 16px;
      border: 1px solid var(--border-subtle);
      padding: 10px 10px 9px;
      position: relative;
      background: linear-gradient(160deg, rgba(255,255,255,0.06), rgba(0,0,0,0.9));
    }

    .tor-node-label {
      font-size: 0.75rem;
      text-transform: uppercase;
      letter-spacing: 0.16em;
      color: var(--accent-soft);
      margin-bottom: 4px;
    }

    .tor-node-main {
      font-size: 0.85rem;
      margin-bottom: 4px;
    }

    .tor-node-sub {
      font-size: 0.78rem;
      color: var(--text-muted);
    }

    .tor-arrow {
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 0.9rem;
      color: var(--accent-soft);
    }

    .stat-row {
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
      margin-top: 12px;
    }

    .stat-chip {
      border-radius: var(--radius-pill);
      border: 1px solid var(--border-subtle);
      padding: 6px 10px;
      font-size: 0.78rem;
      color: var(--accent-soft);
      display: inline-flex;
      align-items: center;
      gap: 6px;
      background: rgba(5,7,8,0.8);
    }

    .stat-chip strong {
      color: var(--text-main);
    }

    /* Results table */
    .results-table-wrapper {
      margin-top: 12px;
      border-radius: 18px;
      border: 1px solid var(--border-subtle);
      overflow: hidden;
      background: radial-gradient(circle at top, #16191e, #050708);
      box-shadow: var(--shadow-soft);
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.83rem;
    }

    th, td {
      padding: 9px 12px;
      white-space: nowrap;
    }

    th {
      text-align: left;
      text-transform: uppercase;
      letter-spacing: 0.12em;
      font-size: 0.74rem;
      color: var(--text-muted);
      border-bottom: 1px solid var(--border-subtle);
      background: rgba(5,7,8,0.95);
    }

    tbody tr:nth-child(odd) {
      background: rgba(255,255,255,0.01);
    }

    tbody tr:hover {
      background: rgba(255,255,255,0.02);
    }

    .cell-human {
      color: #f5f5f5;
      font-weight: 500;
    }

    .cell-gap {
      color: var(--accent-soft);
      font-size: 0.8rem;
    }

    /* Code block */
    pre {
      font-family: "JetBrains Mono", ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 0.8rem;
      background: #050607;
      border-radius: 12px;
      border: 1px solid var(--border-subtle);
      padding: 14px 16px;
      overflow-x: auto;
      color: #d4d4d4;
    }

    /* Footer */
    .footer {
      margin-top: 56px;
      padding-top: 20px;
      border-top: 1px solid var(--border-subtle);
      font-size: 0.8rem;
      color: var(--text-muted);
      display: flex;
      flex-wrap: wrap;
      justify-content: space-between;
      gap: 10px;
    }

    /* Fade-in on scroll */
    .reveal {
      opacity: 0;
      transform: translateY(18px);
      transition: opacity 0.6s ease-out, transform 0.6s ease-out;
    }

    .reveal.visible {
      opacity: 1;
      transform: translateY(0);
    }
  </style>
</head>
<body>
  <!-- NAVBAR -->
  <header class="nav">
    <div class="nav-left">
      <div class="nav-dot"></div>
      <div class="nav-title">Fire360 ¬∑ NeurIPS 2025</div>
    </div>
    <nav class="nav-links">
      <a href="#motivation">Motivation</a>
      <a href="#dataset">Dataset</a>
      <a href="#tasks">Tasks</a>
      <a href="#tor">TOR</a>
      <a href="#results">Results</a>
      <a href="#resources">Resources</a>
    </nav>
  </header>

  <main class="page">
    <!-- HERO -->
    <section class="hero reveal">
      <div>
        <div class="badge">
          <span class="badge-dot"></span>
          <span>Benchmark ¬∑ 360¬∞ Firefighting</span>
        </div>
        <h1 class="hero-title">
          Fire360: Robust Perception & Episodic Memory in Degraded 360¬∞ Firefighting Video
        </h1>
        <p class="hero-subtitle">
          A 50-hour benchmark of professional fire training sessions that probes spatial reasoning,
          safety-critical procedures, and transformation-invariant retrieval under dense smoke,
          low light, and structural deformation.
        </p>
        <p class="hero-authors">
          <strong>Aditi Tiwari</strong>, Farzaneh Masoud, Dac Trong Nguyen, Jill Kraft, Heng Ji, Klara Nahrstedt<br/>
          University of Illinois Urbana‚ÄìChampaign ¬∑ Illinois Fire Service Institute
        </p>
        <div class="hero-buttons">
          <a href="PAPER_LINK_HERE" class="btn" target="_blank" rel="noreferrer">
            <span class="icon">‚¨á</span>
            <span>Paper (PDF)</span>
          </a>
          <a href="DATASET_LINK_HERE" class="btn btn-secondary" target="_blank" rel="noreferrer">
            <span class="icon">üéû</span>
            <span>Dataset & Toolkit</span>
          </a>
          <a href="CODE_REPO_LINK_HERE" class="btn btn-secondary" target="_blank" rel="noreferrer">
            <span class="icon">üíª</span>
            <span>Code (Coming Soon)</span>
          </a>
        </div>
        <div class="hero-meta">
          228 videos ¬∑ 5 benchmark tasks ¬∑ 43.7% human‚Äìmodel gap on transformed object retrieval
        </div>
      </div>

      <div class="hero-right">
        <div class="hero-card">
          <div class="hero-card-header">
            <span>Fire360 scene representation</span>
            <span class="hero-card-chip">360¬∞ Panoramic ¬∑ Equirectangular</span>
          </div>
          <div class="hero-view">
            <div class="hero-view-inner">
              <div class="hero-view-grid"></div>
              <div class="hero-view-scan"></div>
            </div>
            <div class="hero-view-label">
              Degraded Visibility ¬∑ Smoke Level 4
            </div>
          </div>
          <div class="hero-stats">
            <div class="hero-stat-item">
              <div class="hero-stat-label">Duration</div>
              <div class="hero-stat-value">50 hours</div>
            </div>
            <div class="hero-stat-item">
              <div class="hero-stat-label">Videos</div>
              <div class="hero-stat-value">228</div>
            </div>
            <div class="hero-stat-item">
              <div class="hero-stat-label">Tasks</div>
              <div class="hero-stat-value">VQA ¬∑ TAC ¬∑ Loc ¬∑ Safety ¬∑ TOR</div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- MOTIVATION -->
    <section id="motivation" class="reveal">
      <div class="section-header">
        <h2 class="section-title">Motivation</h2>
        <p class="section-tagline">
          Firefighters operate in dense smoke, collapsing structures, and thermal distortion. Fire360
          asks whether modern AI can maintain reliable perception and memory when visibility is
          worst‚Äîexactly when it matters most.
        </p>
      </div>

      <div class="columns-two">
        <div class="section-body">
          <p>
            In 2023, over sixty thousand U.S. firefighters were injured on duty, with many incidents
            linked to breakdowns in situational perception under low visibility. Traditional
            datasets focus on clean 2D imagery, short clips, or synthetic simulations, and rarely
            capture the compound stressors of real incident scenes: smoke, occlusion, glare,
            structural deformation, and multi-agent coordination.
          </p>
          <br/>
          <p>
            Fire360 is built from professionally recorded 360¬∞ training sessions, annotated with
            actions, objects, and degradation metadata. It targets the capabilities that human
            responders rely on most: robust spatial grounding, temporal understanding of protocols,
            safety-critical reasoning, and episodic memory that can recognize gear after it has
            melted, charred, or disappeared into smoke.
          </p>
        </div>

        <div class="card">
          <div class="card-subtitle">Capability Gap</div>
          <div class="card-title">Where models fail and humans do not</div>
          <p class="card-text">
            Human experts retain over <strong>80% accuracy</strong> across all degradation levels on Fire360,
            and <strong>83.5%</strong> on the transformed object retrieval task. State-of-the-art multimodal
            models fall to <strong>~40%</strong> under the same conditions, misidentifying melted helmets as
            pipes and charred hoses as background debris.
          </p>
          <br/>
          <div class="pill">
            <span class="pill-dot"></span>
            <span>Benchmarking robust perception where failure is operational, not theoretical.</span>
          </div>
        </div>
      </div>
    </section>

    <!-- DATASET -->
    <section id="dataset" class="reveal">
      <div class="section-header">
        <h2 class="section-title">Dataset Overview</h2>
        <p class="section-tagline">
          360¬∞ firefighter training video with expert-verified annotations for actions, objects,
          and environmental degradation.
        </p>
      </div>
      <div class="columns-two">
        <div class="section-body">
          <p>
            Fire360 comprises <strong>228</strong> emergency response videos (approximately
            <strong>180,000 seconds</strong> / 50 hours) recorded at a professional firefighter training
            institute. Cameras capture both tripod-mounted third-person views and helmet-mounted
            or handheld egocentric perspectives, across indoor rescues and outdoor suppression
            scenarios, day and night.
          </p>
          <br/>
          <p>
            Each video is annotated with temporal action segments, object bounding boxes, and
            environmental metadata such as smoke level, visibility, heat distortion, and
            multi-agent interaction. Annotations are verified by certified instructors, with
            strong inter-annotator agreement on both actions and objects.
          </p>
        </div>

        <div class="card">
          <div class="card-subtitle">Composition</div>
          <div class="card-title">Scene diversity & annotation schema</div>
          <ul class="card-text" style="list-style: none; padding-left: 0;">
            <li>‚Ä¢ 43.9% indoor, 56.1% outdoor; day, night, and mixed light.</li>
            <li>‚Ä¢ 8 high-risk actions (e.g., civilian carry, ladder climb, window break).</li>
            <li>‚Ä¢ 6 safety-critical object classes (e.g., civilian, fire, gas mask, helmet).</li>
            <li>‚Ä¢ Smoke levels 1‚Äì5, heat and visibility tags, structural hazards.</li>
            <li>‚Ä¢ Train/val/test split: 60% / 20% / 20%, stratified by degradation.</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- TASKS -->
    <section id="tasks" class="reveal">
      <div class="section-header">
        <h2 class="section-title">Benchmark Tasks</h2>
        <p class="section-tagline">
          Five tasks probe complementary capabilities: spatial grounding, temporal understanding,
          safety reasoning, localization under distortion, and transformation-invariant retrieval.
        </p>
      </div>

      <div class="task-grid">
        <div class="task-card">
          <div class="task-title">360¬∞ Visual Question Answering (VQA)</div>
          <div class="task-meta">Spatial reasoning ¬∑ Checklist-style queries</div>
          <p class="task-text">
            Models answer expert-authored questions about object presence, protocol adherence,
            and visibility in full panoramic frames (e.g., ‚ÄúIs the exit door visible through
            the smoke?‚Äù).
          </p>
          <div class="task-chip-row">
            <span class="pill">Metric: Top-1 accuracy</span>
            <span class="pill">Human: 91.4%</span>
            <span class="pill">GPT-4o: 53.8%</span>
          </div>
        </div>

        <div class="task-card">
          <div class="task-title">Temporal Action Captioning</div>
          <div class="task-meta">Incident summarization ¬∑ 10‚Äì20s clips</div>
          <p class="task-text">
            Given a short degraded clip, models must produce grounded action descriptions
            (e.g., ‚ÄúResponder breaks glass to enter a burning room.‚Äù).
          </p>
          <div class="task-chip-row">
            <span class="pill">Metric: BLEU-4</span>
            <span class="pill">Human: 0.85</span>
            <span class="pill">GLaMM-7B: 0.341</span>
          </div>
        </div>

        <div class="task-card">
          <div class="task-title">Object Localization under Distortion</div>
          <div class="task-meta">Detection ¬∑ Smoke & geometric stretch</div>
          <p class="task-text">
            Category-agnostic localization of safety gear (SCBA, helmets, hoses) in 360¬∞
            equirectangular frames, evaluated via mean IoU.
          </p>
          <div class="task-chip-row">
            <span class="pill">Metric: Mean IoU</span>
            <span class="pill">Human: 85.2%</span>
            <span class="pill">Grounding DINO: 38.4%</span>
          </div>
        </div>

        <div class="task-card">
          <div class="task-title">Safety-Critical Reasoning</div>
          <div class="task-meta">Procedural compliance ¬∑ Safe/unsafe + rationale</div>
          <p class="task-text">
            Models must label frames or clips as safe or unsafe and justify their decision,
            aligned with firefighter safety checklists (e.g., two-point ladder contact, sealed
            gas mask).
          </p>
          <div class="task-chip-row">
            <span class="pill">Metric: Checklist accuracy</span>
            <span class="pill">Human: 94.6%</span>
            <span class="pill">GPT-4o: 28.9%</span>
          </div>
        </div>

        <div class="task-card">
          <div class="task-title">Transformed Object Retrieval (TOR)</div>
          <div class="task-meta">Episodic memory ¬∑ Core contribution</div>
          <p class="task-text">
            Given a pristine exemplar (e.g., clean helmet), models must retrieve the same object
            after irreversible fire damage in a different, unpaired scene.
          </p>
          <div class="task-chip-row">
            <span class="pill">Metric: Top-1 accuracy (IoU ‚â• 0.5)</span>
            <span class="pill">Human: 83.5%</span>
            <span class="pill">GPT-4o: 39.8%</span>
          </div>
        </div>

        <div class="task-card">
          <div class="task-title">Extended Model Families</div>
          <div class="task-meta">Robustness across architectures</div>
          <p class="task-text">
            Fire360 evaluates instruction-tuned VLMs, captioners, open-vocabulary detectors,
            and safety-specialized classifiers. All exhibit consistent degradation-induced
            failures, independent of architecture.
          </p>
          <div class="task-chip-row">
            <span class="pill">Avg human‚Äìmodel gap &gt; 50%</span>
            <span class="pill">17 models evaluated</span>
          </div>
        </div>
      </div>
    </section>

    <!-- TOR SECTION -->
    <section id="tor" class="tor-section reveal">
      <div class="section-header">
        <h2 class="section-title">Transformed Object Retrieval (TOR)</h2>
        <p class="section-tagline">
          Can a model recognize a firefighter‚Äôs gear once flame, smoke, and debris have warped it
          beyond recognition‚Äîwithout any before/after pairing?
        </p>
      </div>

      <div class="tor-layout">
        <div>
          <div class="tor-diagram">
            <div class="tor-diagram-header">
              <span>Task formulation</span>
              <span class="pill"><span class="pill-dot"></span> Episodic memory under structural deformation</span>
            </div>

            <div class="tor-flow">
              <div class="tor-node">
                <div class="tor-node-label">Reference</div>
                <div class="tor-node-main">Pristine exemplar</div>
                <div class="tor-node-sub">
                  Clean view of a helmet, SCBA, or hose captured outside the fire zone.
                </div>
              </div>

              <div class="tor-arrow">
                ‚ü∂
              </div>

              <div class="tor-node">
                <div class="tor-node-label">Retrieval Scene</div>
                <div class="tor-node-main">Unpaired 360¬∞ frame</div>
                <div class="tor-node-sub">
                  Separate fire scene: dense smoke, occlusion, charring, geometric distortion.
                  No temporal or spatial continuity.
                </div>
              </div>
            </div>

            <div class="tor-flow" style="margin-top: 10px;">
              <div class="tor-node">
                <div class="tor-node-label">Candidates</div>
                <div class="tor-node-main">Region proposals</div>
                <div class="tor-node-sub">
                  Grounding DINO proposes ~36 regions per frame; a vision-language encoder
                  scores similarity against the exemplar.
                </div>
              </div>

              <div class="tor-arrow">
                ‚ü∂
              </div>

              <div class="tor-node">
                <div class="tor-node-label">Success condition</div>
                <div class="tor-node-main">Top-1 retrieval</div>
                <div class="tor-node-sub">
                  Retrieved region overlaps expert-annotated target with IoU ‚â• 0.5.
                </div>
              </div>
            </div>

            <div class="stat-row">
              <div class="stat-chip">
                <strong>154</strong> degraded targets ¬∑ <strong>50</strong> exemplars ¬∑ <strong>20</strong> categories
              </div>
              <div class="stat-chip">
                Common failure: pipes mistaken for melted helmets; hoses lost in debris.
              </div>
            </div>
          </div>
        </div>

        <div class="card">
          <div class="card-subtitle">Findings</div>
          <div class="card-title">A 43.7-point gap in transformation-invariant retrieval</div>
          <p class="card-text">
            TOR exposes a key missing capability in current models: maintaining object identity
            through irreversible physical change without seeing the transformation unfold.
            Despite strong performance on clean retrieval benchmarks, models collapse when
            texture, shape, or context are heavily distorted by fire.
          </p>
          <br/>
          <ul class="card-text" style="list-style: none; padding-left: 0;">
            <li>‚Ä¢ GPT-4o: <strong>39.8%</strong> top-1 TOR accuracy vs. human <strong>83.5%</strong>.</li>
            <li>‚Ä¢ CLIP / BLIP-2: lower performance, heavily distracted by similar shapes.</li>
            <li>‚Ä¢ Performance is most brittle near equirectangular poles with high distortion.</li>
          </ul>
          <br/>
          <p class="card-text">
            TOR generalizes beyond firefighting: medical imaging (tracking deformed tissue),
            post-disaster insurance workflows (identifying burnt items), and manufacturing
            (tracing visually altered components).
          </p>
        </div>
      </div>
    </section>

    <!-- RESULTS -->
    <section id="results" class="reveal">
      <div class="section-header">
        <h2 class="section-title">Model Performance</h2>
        <p class="section-tagline">
          Fire360 surfaces consistent degradation-induced failures across VLMs, captioners,
          detectors, and safety classifiers‚Äîeven when architectures differ.
        </p>
      </div>

      <div class="results-table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Task</th>
              <th>Metric</th>
              <th>Best Model</th>
              <th>Model Score</th>
              <th>Human</th>
              <th>Gap</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>360¬∞ VQA</td>
              <td>Top-1 accuracy</td>
              <td>GPT-4o</td>
              <td>53.8%</td>
              <td class="cell-human">91.4%</td>
              <td class="cell-gap">‚àí37.6 pts</td>
            </tr>
            <tr>
              <td>Temporal Action Captioning</td>
              <td>BLEU-4</td>
              <td>GLaMM-7B</td>
              <td>0.341</td>
              <td class="cell-human">0.85</td>
              <td class="cell-gap">‚àí0.509</td>
            </tr>
            <tr>
              <td>Object Localization</td>
              <td>Mean IoU</td>
              <td>Grounding DINO</td>
              <td>38.4%</td>
              <td class="cell-human">85.2%</td>
              <td class="cell-gap">‚àí46.8 pts</td>
            </tr>
            <tr>
              <td>Safety-Critical Reasoning</td>
              <td>Checklist accuracy</td>
              <td>Qwen-VL</td>
              <td>32.5%</td>
              <td class="cell-human">94.6%</td>
              <td class="cell-gap">‚àí62.1 pts</td>
            </tr>
            <tr>
              <td>Transformed Object Retrieval (TOR)</td>
              <td>Top-1 accuracy</td>
              <td>GPT-4o</td>
              <td>39.8%</td>
              <td class="cell-human">83.5%</td>
              <td class="cell-gap">‚àí43.7 pts</td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <!-- RESOURCES / TOOLKIT -->
    <section id="resources" class="reveal">
      <div class="section-header">
        <h2 class="section-title">Toolkit & Resources</h2>
        <p class="section-tagline">
          Preprocessing, evaluation scripts, and prompt templates to reproduce all experiments
          and extend Fire360 to new models.
        </p>
      </div>

      <div class="columns-two">
        <div class="card">
          <div class="card-subtitle">Toolkit</div>
          <div class="card-title">What we provide</div>
          <ul class="card-text" style="list-style: none; padding-left: 0;">
            <li>‚Ä¢ Equirectangular ‚Üí rectilinear conversion utilities (OpenCV-based).</li>
            <li>‚Ä¢ Evaluation scripts for all five tasks, including degradation-aware splits.</li>
            <li>‚Ä¢ Grounding DINO configuration for proposal generation in TOR.</li>
            <li>‚Ä¢ Prompt templates and checklists for VQA, Safety, and TOR.</li>
            <li>‚Ä¢ Example outputs and confidence intervals for model comparison.</li>
          </ul>
          <br/>
          <p class="card-text">
            Full test set evaluation requires ~4 GPU-hours on A40s or ~2.5 on A100s; the raw
            dataset occupies ~400GB.
          </p>
        </div>

        <div>
          <div class="card" style="margin-bottom: 18px;">
            <div class="card-subtitle">Ethics & Use</div>
            <div class="card-title">Responsible release</div>
            <p class="card-text">
              All videos document professional training drills conducted with informed consent.
              No personally identifiable information is included; all participants appear in
              protective gear. The dataset is released for research on safety-critical perception
              and robust multimodal reasoning, with license terms that prohibit surveillance,
              behavioral profiling, or non-consensual monitoring.
            </p>
          </div>

          <div class="card">
            <div class="card-subtitle">Citation</div>
            <div class="card-title">BibTeX</div>
            <pre>
@inproceedings{tiwari2025fire360,
  title     = {Fire360: A Benchmark for Robust Perception and Episodic Memory
               in Degraded 360{\degree} Firefighting Video},
  author    = {Tiwari, Aditi and Masoud, Farzaneh and Nguyen, Dac Trong and
               Kraft, Jill and Ji, Heng and Nahrstedt, Klara},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2025}
}</pre>
          </div>
        </div>
      </div>
    </section>

    <!-- FOOTER -->
    <footer class="footer">
      <div>
        Fire360 ¬∑ NeurIPS 2025 &nbsp;¬∑&nbsp; UIUC ¬∑ Illinois Fire Service Institute
      </div>
      <div>
        Contact: <a href="mailto:aditit5@illinois.edu">aditit5@illinois.edu</a>
      </div>
    </footer>
  </main>

  <script>
    // Reveal elements on scroll (Intersection Observer)
    const reveals = document.querySelectorAll('.reveal');

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          entry.target.classList.add('visible');
          observer.unobserve(entry.target);
        }
      });
    }, { threshold: 0.15 });

    reveals.forEach(el => observer.observe(el));
  </script>
</body>
</html>
